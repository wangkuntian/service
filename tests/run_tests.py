#!/usr/bin/env python3
"""æµ‹è¯•è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¾¿æ·çš„æ–¹å¼æ¥è¿è¡Œä¸åŒç±»å‹çš„æµ‹è¯•ï¼š
- å•å…ƒæµ‹è¯•
- æ€§èƒ½æµ‹è¯•
- é›†æˆæµ‹è¯•
- å…¨éƒ¨æµ‹è¯•

ä½¿ç”¨æ–¹æ³•:
    python run_tests.py [é€‰é¡¹]

é€‰é¡¹:
    --unit          åªè¿è¡Œå•å…ƒæµ‹è¯•
    --performance   åªè¿è¡Œæ€§èƒ½æµ‹è¯•
    --integration   åªè¿è¡Œé›†æˆæµ‹è¯•
    --slow          åŒ…å«æ…¢é€Ÿæµ‹è¯•
    --all           è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆé»˜è®¤ï¼‰
    --coverage      ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
    --benchmark     è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
    --help          æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
"""

import sys
import subprocess
import argparse
import os
from pathlib import Path


def run_command(cmd, description=''):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f'\nğŸš€ {description}')
    print(f'æ‰§è¡Œå‘½ä»¤: {" ".join(cmd)}')
    print('-' * 60)

    try:
        result = subprocess.run(cmd, capture_output=False, text=True)
        return result.returncode == 0
    except Exception as e:
        print(f'âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}')
        return False


def main():
    parser = argparse.ArgumentParser(
        description='ServiceLauncher æµ‹è¯•è¿è¡Œè„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    parser.add_argument('--unit', action='store_true', help='åªè¿è¡Œå•å…ƒæµ‹è¯•')
    parser.add_argument(
        '--performance', action='store_true', help='åªè¿è¡Œæ€§èƒ½æµ‹è¯•'
    )
    parser.add_argument(
        '--integration', action='store_true', help='åªè¿è¡Œé›†æˆæµ‹è¯•'
    )
    parser.add_argument('--slow', action='store_true', help='åŒ…å«æ…¢é€Ÿæµ‹è¯•')
    parser.add_argument(
        '--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆé»˜è®¤ï¼‰'
    )
    parser.add_argument(
        '--coverage', action='store_true', help='ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š'
    )
    parser.add_argument(
        '--benchmark', action='store_true', help='åªè¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•'
    )
    parser.add_argument(
        '--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º'
    )
    parser.add_argument(
        '--parallel', '-j', type=int, default=1, help='å¹¶è¡Œè¿è¡Œæµ‹è¯•çš„è¿›ç¨‹æ•°'
    )

    args = parser.parse_args()

    # ç¡®ä¿åœ¨testsç›®å½•ä¸­è¿è¡Œ
    script_dir = Path(__file__).parent
    os.chdir(script_dir)

    # åŸºç¡€pytestå‘½ä»¤
    base_cmd = ['python', '-m', 'pytest']

    # æ·»åŠ è¯¦ç»†è¾“å‡º
    if args.verbose:
        base_cmd.extend(['-v', '-s'])

    # æ·»åŠ å¹¶è¡Œå¤„ç†
    if args.parallel > 1:
        base_cmd.extend(['-n', str(args.parallel)])

    # ç¡®å®šè¦è¿è¡Œçš„æµ‹è¯•ç±»å‹
    test_types = []

    if args.unit:
        test_types.append('unit')
    if args.performance:
        test_types.append('performance')
    if args.integration:
        test_types.append('integration')
    if args.benchmark:
        test_types.append('performance')
        # åªè¿è¡ŒåŸºå‡†æµ‹è¯•
        base_cmd.extend(['-k', 'benchmark'])

    # å¦‚æœæ²¡æœ‰æŒ‡å®šç‰¹å®šç±»å‹ï¼Œæˆ–æŒ‡å®šäº†--allï¼Œè¿è¡Œæ‰€æœ‰æµ‹è¯•
    if not test_types or args.all:
        test_types = ['unit', 'performance', 'integration']

    # æ„å»ºæ ‡è®°è¡¨è¾¾å¼
    if test_types and not args.all:
        mark_expr = ' or '.join(test_types)
        base_cmd.extend(['-m', mark_expr])

    # å¤„ç†æ…¢é€Ÿæµ‹è¯•
    if not args.slow and not args.benchmark:
        if '-m' in base_cmd:
            # å¦‚æœå·²ç»æœ‰æ ‡è®°è¡¨è¾¾å¼ï¼Œæ·»åŠ æ’é™¤slowçš„æ¡ä»¶
            mark_index = base_cmd.index('-m') + 1
            current_marks = base_cmd[mark_index]
            base_cmd[mark_index] = f'({current_marks}) and not slow'
        else:
            # å¦‚æœæ²¡æœ‰æ ‡è®°è¡¨è¾¾å¼ï¼Œåªæ’é™¤slow
            base_cmd.extend(['-m', 'not slow'])

    # æ·»åŠ è¦†ç›–ç‡
    if args.coverage:
        base_cmd.extend(
            [
                '--cov=service.launchers.service_launcher',
                '--cov-report=html:htmlcov',
                '--cov-report=term-missing',
                '--cov-report=xml',
            ]
        )

    # è¿è¡Œæµ‹è¯•
    success = True

    print('=' * 60)
    print('ğŸ§ª ServiceLauncher æµ‹è¯•å¥—ä»¶')
    print('=' * 60)

    if args.unit or args.all:
        print('\nğŸ“‹ è¿è¡Œå•å…ƒæµ‹è¯•...')
        unit_cmd = base_cmd + ['test_service_launcher.py']
        if not args.all:
            unit_cmd.extend(
                ['-m', 'unit and not slow']
            ) if not args.slow else unit_cmd.extend(['-m', 'unit'])
        success &= run_command(unit_cmd, 'å•å…ƒæµ‹è¯•')

    if args.performance or args.benchmark or args.all:
        print('\nâš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...')
        perf_cmd = base_cmd + ['test_service_launcher_performance.py']
        if args.benchmark:
            perf_cmd.extend(['-k', 'benchmark'])
        elif not args.all:
            perf_cmd.extend(
                ['-m', 'performance and not slow']
            ) if not args.slow else perf_cmd.extend(['-m', 'performance'])
        success &= run_command(perf_cmd, 'æ€§èƒ½æµ‹è¯•')

    if args.integration or args.all:
        print('\nğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...')
        int_cmd = base_cmd + ['-m', 'integration']
        if not args.slow:
            int_cmd = base_cmd + ['-m', 'integration and not slow']
        success &= run_command(int_cmd, 'é›†æˆæµ‹è¯•')

    # æ˜¾ç¤ºç»“æœæ‘˜è¦
    print('\n' + '=' * 60)
    if success:
        print('âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼')

        if args.coverage:
            print('\nğŸ“Š è¦†ç›–ç‡æŠ¥å‘Šå·²ç”Ÿæˆ:')
            print('   - HTMLæŠ¥å‘Š: htmlcov/index.html')
            print('   - XMLæŠ¥å‘Š: coverage.xml')

        if args.performance or args.benchmark:
            print('\nâš¡ æ€§èƒ½æµ‹è¯•å®Œæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡è¾“å‡º')
    else:
        print('âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡ºäº†è§£è¯¦æƒ…')
        return 1

    print('=' * 60)
    return 0


if __name__ == '__main__':
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print('\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­')
        sys.exit(1)
    except Exception as e:
        print(f'\n\nâŒ è¿è¡Œæµ‹è¯•æ—¶å‘ç”Ÿé”™è¯¯: {e}')
        sys.exit(1)
#!/usr/bin/env python3
"""æµ‹è¯•è¿è¡Œè„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†å¤šç§è¿è¡ŒProcessLauncheræµ‹è¯•çš„æ–¹å¼ï¼š
- è¿è¡Œæ‰€æœ‰æµ‹è¯•
- åªè¿è¡Œå•å…ƒæµ‹è¯•
- åªè¿è¡Œæ€§èƒ½æµ‹è¯•
- è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•
- ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
"""

import argparse
import os
import subprocess
import sys
from pathlib import Path


def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºæè¿°"""
    print(f"\n{'='*60}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {' '.join(cmd)}")
    print(f"{'='*60}")
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.stdout:
        print("æ ‡å‡†è¾“å‡º:")
        print(result.stdout)
    
    if result.stderr:
        print("é”™è¯¯è¾“å‡º:")
        print(result.stderr)
    
    if result.returncode != 0:
        print(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
        return False
    
    print("å‘½ä»¤æ‰§è¡ŒæˆåŠŸ!")
    return True


def run_unit_tests():
    """è¿è¡Œå•å…ƒæµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest",
        "tests/test_process_launcher.py",
        "-v",
        "--tb=short",
        "-m", "not performance"
    ]
    return run_command(cmd, "è¿è¡ŒProcessLauncherå•å…ƒæµ‹è¯•")


def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest", 
        "tests/test_process_launcher_performance.py",
        "-v",
        "--tb=short",
        "-s"  # æ˜¾ç¤ºprintè¾“å‡º
    ]
    return run_command(cmd, "è¿è¡ŒProcessLauncheræ€§èƒ½æµ‹è¯•")


def run_optimized_performance_tests():
    """è¿è¡Œå†…å­˜ä¼˜åŒ–çš„æ€§èƒ½æµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest", 
        "tests/test_process_launcher_performance_optimized.py",
        "-v",
        "--tb=short",
        "-s"
    ]
    return run_command(cmd, "è¿è¡Œå†…å­˜ä¼˜åŒ–çš„ProcessLauncheræ€§èƒ½æµ‹è¯•")


def run_memory_diagnostic():
    """è¿è¡Œå†…å­˜è¯Šæ–­"""
    cmd = [
        "python", "tests/memory_diagnostic.py", "--test", "performance"
    ]
    return run_command(cmd, "è¿è¡Œå†…å­˜è¯Šæ–­åˆ†æ")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest",
        "tests/",
        "-v",
        "--tb=short"
    ]
    return run_command(cmd, "è¿è¡Œæ‰€æœ‰ProcessLauncheræµ‹è¯•")


def run_tests_with_coverage():
    """è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å®‰è£…äº†pytest-cov
    try:
        import pytest_cov
    except ImportError:
        print("æœªå®‰è£…pytest-covï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.run([sys.executable, "-m", "pip", "install", "pytest-cov"])
    
    cmd = [
        "python", "-m", "pytest",
        "tests/",
        "--cov=src/service/launchers/process_launcher",
        "--cov-report=html:tests/htmlcov",
        "--cov-report=term-missing",
        "--cov-report=xml:tests/coverage.xml",
        "-v"
    ]
    return run_command(cmd, "è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•")


def run_specific_test(test_name):
    """è¿è¡Œç‰¹å®šæµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest",
        "-k", test_name,
        "-v",
        "--tb=short",
        "-s"
    ]
    return run_command(cmd, f"è¿è¡Œç‰¹å®šæµ‹è¯•: {test_name}")


def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å®‰è£…äº†pytest-html
    try:
        import pytest_html
    except ImportError:
        print("æœªå®‰è£…pytest-htmlï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.run([sys.executable, "-m", "pip", "install", "pytest-html"])
    
    cmd = [
        "python", "-m", "pytest",
        "tests/",
        "--html=tests/report.html",
        "--self-contained-html",
        "-v"
    ]
    return run_command(cmd, "ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š")


def check_dependencies():
    """æ£€æŸ¥æµ‹è¯•ä¾èµ–"""
    print("æ£€æŸ¥æµ‹è¯•ä¾èµ–...")
    
    required_packages = [
        "pytest",
        "pytest-mock", 
        "pytest-asyncio",
        "psutil"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"âœ“ {package} å·²å®‰è£…")
        except ImportError:
            print(f"âœ— {package} æœªå®‰è£…")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    print("\næ‰€æœ‰ä¾èµ–å·²æ»¡è¶³!")
    return True


def benchmark_tests():
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    cmd = [
        "python", "-m", "pytest", 
        "tests/test_process_launcher_performance.py::test_performance_benchmarks",
        "-v",
        "-s"
    ]
    return run_command(cmd, "è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="ProcessLauncheræµ‹è¯•è¿è¡Œè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python run_tests.py --all              # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python run_tests.py --unit             # åªè¿è¡Œå•å…ƒæµ‹è¯•
  python run_tests.py --performance      # åªè¿è¡Œæ€§èƒ½æµ‹è¯•
  python run_tests.py --optimized        # è¿è¡Œå†…å­˜ä¼˜åŒ–æ€§èƒ½æµ‹è¯•
  python run_tests.py --coverage         # è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•
  python run_tests.py --report           # ç”ŸæˆHTMLæŠ¥å‘Š
  python run_tests.py --benchmark        # è¿è¡ŒåŸºå‡†æµ‹è¯•
  python run_tests.py --diagnostic       # è¿è¡Œå†…å­˜è¯Šæ–­
  python run_tests.py --test "signal"    # è¿è¡ŒåŒ…å«"signal"çš„æµ‹è¯•
  python run_tests.py --check            # æ£€æŸ¥ä¾èµ–
        """
    )
    
    parser.add_argument(
        "--all", 
        action="store_true", 
        help="è¿è¡Œæ‰€æœ‰æµ‹è¯•"
    )
    parser.add_argument(
        "--unit", 
        action="store_true", 
        help="åªè¿è¡Œå•å…ƒæµ‹è¯•"
    )
    parser.add_argument(
        "--performance", 
        action="store_true", 
        help="åªè¿è¡Œæ€§èƒ½æµ‹è¯•"
    )
    parser.add_argument(
        "--optimized", 
        action="store_true", 
        help="è¿è¡Œå†…å­˜ä¼˜åŒ–æ€§èƒ½æµ‹è¯•"
    )
    parser.add_argument(
        "--coverage", 
        action="store_true", 
        help="è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•"
    )
    parser.add_argument(
        "--report", 
        action="store_true", 
        help="ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"
    )
    parser.add_argument(
        "--benchmark", 
        action="store_true", 
        help="è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"
    )
    parser.add_argument(
        "--diagnostic", 
        action="store_true", 
        help="è¿è¡Œå†…å­˜è¯Šæ–­"
    )
    parser.add_argument(
        "--test", 
        type=str, 
        help="è¿è¡Œç‰¹å®šæµ‹è¯•ï¼ˆæŒ‰åç§°åŒ¹é…ï¼‰"
    )
    parser.add_argument(
        "--check", 
        action="store_true", 
        help="æ£€æŸ¥æµ‹è¯•ä¾èµ–"
    )
    
    args = parser.parse_args()
    
    # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).parent.parent
    os.chdir(project_root)
    
    success = True
    
    if args.check:
        success = check_dependencies()
    elif args.unit:
        success = run_unit_tests()
    elif args.performance:
        success = run_performance_tests()
    elif args.optimized:
        success = run_optimized_performance_tests()
    elif args.coverage:
        success = run_tests_with_coverage()
    elif args.report:
        success = generate_test_report()
    elif args.benchmark:
        success = benchmark_tests()
    elif args.diagnostic:
        success = run_memory_diagnostic()
    elif args.test:
        success = run_specific_test(args.test)
    elif args.all:
        success = run_all_tests()
    else:
        # é»˜è®¤è¿è¡Œå•å…ƒæµ‹è¯•
        print("æœªæŒ‡å®šæµ‹è¯•ç±»å‹ï¼Œè¿è¡Œå•å…ƒæµ‹è¯•...")
        success = run_unit_tests()
    
    if success:
        print(f"\n{'='*60}")
        print("æµ‹è¯•æ‰§è¡Œå®Œæˆ!")
        print(f"{'='*60}")
        sys.exit(0)
    else:
        print(f"\n{'='*60}")
        print("æµ‹è¯•æ‰§è¡Œå¤±è´¥!")
        print(f"{'='*60}")
        sys.exit(1)


if __name__ == "__main__":
    main() 